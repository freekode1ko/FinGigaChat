"""–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã function calling"""

import dataclasses
from typing import Any

from aiogram import types
from langchain_core.runnables import RunnableConfig
from langchain_core.runnables.base import Runnable
from langchain_gigachat.chat_models.gigachat import GigaChat
from langchain_openai import ChatOpenAI

from configs.config import giga_scope, giga_credentials
from utils.function_calling.tool_functions.preparing_meeting.config import API_KEY, BASE_URL, AGENT_MODEL, \
    AGENT_MODEL_TYPE, TEMP, MAX_TOKENS


@dataclasses.dataclass
class LanggraphConfig:
    """–ö–ª–∞—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞ langchain"""

    message: types.Message

    def config_to_langgraph_format(self) -> dict[str, dict[str, Any]]:
        """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –∏–∑ –∫–ª–∞—Å—Å–∞ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è langgraph"""
        return {'configurable': self.__dict__}


def parse_runnable_config(config: RunnableConfig) -> LanggraphConfig:
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç RunnableConfig, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–ª—É—á–∞—é—Ç —Ç—É–∑–ª—ã, –≤ dataclass"""
    return LanggraphConfig(
        **{
            k: v
            for k, v in config['configurable'].items()
            if k in LanggraphConfig.__dataclass_fields__ and isinstance(v, LanggraphConfig.__dataclass_fields__[k].type)
        }
    )


async def get_answer_llm(llm: Runnable,
                         system_prompt: str,
                         user_prompt: str,
                         text: str) -> str:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –º–æ–¥–µ–ª—å.

    :param llm: –∏–Ω—Å—Ç–∞–Ω—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ª–ª–º.
    :param system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏.
    :param user_prompt: —à–∞–±–ª–æ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.
    :param text: —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.
    :return: –æ—Ç–≤–µ—Ç –æ—Ç –≥–∏–≥–∞—á–∞—Ç–∞.
    """
    messages = [{"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt.format(text=text)}]
    try:
        response = await llm.ainvoke(messages)
        content = response.content
        return content
    except Exception as e:
        print(f"–û—à–±–∏–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ì–∏–≥–∞—á–∞—Ç–∞: {e}")
        return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ì–∏–≥–∞—á–∞—Ç–∞"


async def send_status_message_for_agent(
        config: RunnableConfig,
        text: str,
        is_start_message: bool = False
):
    """"""
    try:
        message = config['configurable']['message']
        buttons = config['configurable']['buttons']
        message_text = config['configurable']['message_text']
        final_message = config['configurable']['final_message']
        task_text = config['configurable']['task_text']
        tasks_left = config['configurable']['tasks_left']

        message_text.append(f'<b>{text}</b>\n')
        message_text.append(f'<blockquote expandable>{task_text}</blockquote>\n\n')

        await final_message.edit_text(''.join(message_text) + f'ü¶ø–û—Å—Ç–∞–ª–æ—Å—å <b>{tasks_left}</b> —à–∞–≥–æ–≤...', parse_mode='HTML')
    except Exception as e:
        print(e)
        pass


def get_model() -> Runnable:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å

    :return: –º–æ–¥–µ–ª—å –ª–ª–º.
    """
    if AGENT_MODEL == 'gpt':
        llm = ChatOpenAI(model=AGENT_MODEL_TYPE,
                         api_key=API_KEY,
                         base_url=BASE_URL,
                         max_tokens=MAX_TOKENS,
                         temperature=TEMP)
    elif AGENT_MODEL == 'giga':
        llm = GigaChat(verbose=True,
                       credentials=giga_credentials,
                       scope=giga_scope,
                       model=AGENT_MODEL_TYPE,
                       verify_ssl_certs=False,
                       profanity_check=False,
                       temperature=TEMP)
    else:
        raise Exception('Wrong agent model type parameter')
    return llm

